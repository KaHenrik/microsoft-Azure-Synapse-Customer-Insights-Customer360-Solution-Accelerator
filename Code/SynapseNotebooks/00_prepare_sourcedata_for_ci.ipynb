{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\r\n",
        "\r\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "from notebookutils import mssparkutils\r\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read in Data from Azure Data Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "data_lake_account_name = ''\r\n",
        "file_system_name = 'data'\r\n",
        "synapse_workspace_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": true
      },
      "source": [
        "# filenames = ['residents_dynamics','residents_crm','leases','payments','workorders','surveys']\r\n",
        "# for filename in filenames:\r\n",
        "#    spark.sql(\"DROP TABLE c360_data.\" + filename)\r\n",
        "\r\n",
        "# spark.sql(\"DROP DATABASE c360_data\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "try:\r\n",
        "    spark.sql(\"CREATE DATABASE c360_data\")\r\n",
        "except:\r\n",
        "    print(\"Database already exists\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "filename = 'properties'\r\n",
        "filebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/sourcedata/'\r\n",
        "df = spark.read.load(filebasepath + filename.lower() + '.csv', format='csv', header=True,inferSchema=True)\r\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"c360_data.\" + filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "id_columns = ['cid','pid','ptid','uid','utid','lid','sid','paymentid','PostCode']\n",
        "date_columns = ['DateOfBirth']\n",
        "\n",
        "\n",
        "# convert sourcedata csv files to parquet\n",
        "filenames = ['residents_source1','residents_source2','leases','payments','workorders','surveys']\n",
        "\n",
        "filebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/sourcedata/'\n",
        "dfspath = 'https://' + data_lake_account_name + '.dfs.core.windows.net/' + file_system_name + '/sourcedata/'\n",
        "\n",
        "for filename in filenames:\n",
        "    df = spark.read.load(filebasepath + filename.lower() + '.csv', format='csv', header=True,inferSchema=True)\n",
        "\n",
        "   \n",
        "    id_cols = [col for col in df.columns if col in id_columns]\n",
        "    for id_col in id_cols:\n",
        "        if id_col == 'PostCode':\n",
        "            df = df.withColumn(\"PostCode\",lpad(df.PostCode, 5, '0'))  \n",
        "        df = df.withColumn(id_col,df[id_col].cast(StringType()))\n",
        "\n",
        "    date_cols = [col for col in df.columns if col in date_columns]\n",
        "    for date_col in date_cols:\n",
        "        df = df.withColumn(date_col,df[date_col].cast(DateType()))    \n",
        "\n",
        "    df.write.mode(\"overwrite\").saveAsTable(\"c360_data.\" + filename)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "def get_ci_datedefinitions():\r\n",
        "    definitions = []\r\n",
        "    d = {\r\n",
        "            \"traitName\": \"is.formatted\",\r\n",
        "            \"extendsTrait\": \"is\",\r\n",
        "            \"explanation\": \"a root for traits that descibe how data is formatted\"\r\n",
        "        }\r\n",
        "    definitions.append(d)\r\n",
        "    d = {\r\n",
        "            \"traitName\": \"is.formatted.dateTime\",\r\n",
        "            \"extendsTrait\": \"is.formatted\",\r\n",
        "            \"explanation\": \"DateTime data formatted as a string in ISO 8601 format\",\r\n",
        "            \"hasParameters\": [{\r\n",
        "                \"name\": \"format\",\r\n",
        "                \"dataType\": \"stringFormat\",\r\n",
        "                \"defaultValue\": \"MM/DD/YYYY hh:mm\"\r\n",
        "            }]\r\n",
        "        }\r\n",
        "    definitions.append(d)\r\n",
        "    d = {\r\n",
        "            \"traitName\": \"is.formatted.date\",\r\n",
        "            \"extendsTrait\": \"is.formatted\",\r\n",
        "            \"explanation\": \"Date data formatted as a string in ISO 8601 format\",\r\n",
        "            \"hasParameters\": [{\r\n",
        "                \"name\": \"format\",\r\n",
        "                \"dataType\": \"stringFormat\",\r\n",
        "                \"defaultValue\": \"MM/DD/YYYY\"\r\n",
        "            }]\r\n",
        "        }\r\n",
        "    definitions.append(d)\r\n",
        "    d = {\r\n",
        "            \"traitName\": \"is.formatted.time\",\r\n",
        "            \"extendsTrait\": \"is.formatted\",\r\n",
        "            \"explanation\": \"Time data formatted as a string in ISO 8601 format\",\r\n",
        "            \"hasParameters\": [{\r\n",
        "                \"name\": \"format\",\r\n",
        "                \"dataType\": \"stringFormat\",\r\n",
        "                \"defaultValue\": \"hh:mm:ss\"\r\n",
        "            }]\r\n",
        "        }\r\n",
        "    definitions.append(d)\r\n",
        "    return(definitions)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "#create manifest.json\n",
        "\n",
        "filenames = ['residents_source1','residents_source2','leases','payments','workorders','surveys']\n",
        "\n",
        "filebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/synapse/workspaces/{synapse_workspace_name}/warehouse/c360_data.db/'\n",
        "dfspath = 'https://' + data_lake_account_name + '.dfs.core.windows.net/' + file_system_name + '/synapse/workspaces/' + synapse_workspace_name + '/warehouse/c360_data.db/'\n",
        "id_columns = ['cid','pid','ptid','uid','utid','lid','sid','paymentid','CustomerId']\n",
        "curreny_columns = ['amount','RenewalPredictionScore']\n",
        "\n",
        "entities = []\n",
        "\n",
        "for filename in filenames:\n",
        "    df = spark.read.load(filebasepath + filename.lower(), format='parquet', header=True,inferSchema=True)\n",
        "    attributes = []\n",
        "    partitions = []\n",
        "    contents = []\n",
        "    for e in df.schema:\n",
        "        if e.name in id_columns:\n",
        "            edatatype = 'String'\n",
        "        elif e.name in curreny_columns:\n",
        "           edatatype = 'Double'\n",
        "        elif str(e.dataType) == 'StringType':\n",
        "            edatatype = 'String'\n",
        "        elif str(e.dataType) == 'TimestampType':\n",
        "            edatatype = 'DateTime'\n",
        "        elif str(e.dataType) == 'DateType':\n",
        "            edatatype = 'Date'\n",
        "        elif str(e.dataType) == 'IntegerType':\n",
        "            edatatype = 'Int32' #'Integer'\n",
        "        else:\n",
        "            edatatype = 'String'\n",
        "        \n",
        "\n",
        "        if edatatype != 'DateTime':\n",
        "          attr ={\n",
        "              \"name\": e.name,\n",
        "              \"dataFormat\": edatatype,\n",
        "          }\n",
        "        else:\n",
        "          attr ={\n",
        "              \"name\": e.name,\n",
        "              \"appliedTraits\": [\n",
        "                  \"is.formatted.dateTime\"\n",
        "                ],\n",
        "              \"dataFormat\": edatatype\n",
        "          }\n",
        "\n",
        "        attributes.append(attr)\n",
        "        content = {\n",
        "            \"type\" : \"attributeDefinition\",\n",
        "            \"name\" : e.name,\n",
        "            \"parent\" : filename + \"/attributeContext/\" + filename,\n",
        "            \"definition\" : \"resolvedFrom/\" + filename + \"/hasAttributes/\" + e.name,\n",
        "            \"contents\" : [\n",
        "              filename + \"/hasAttributes/\" + e.name\n",
        "            ]\n",
        "          }\n",
        "        contents.append(content)\n",
        "        #break\n",
        "\n",
        "    ent_imports = []\n",
        "    d = {\n",
        "        \"corpusPath\": '/' + filename + \".cdm.json\",\n",
        "        \"moniker\": \"resolvedFrom\"\n",
        "        }\n",
        "    ent_imports.append(d)\n",
        "\n",
        "    ent_definitions = []\n",
        "    d = {\n",
        "      \"entityName\": filename,\n",
        "      \"attributeContext\": {\n",
        "        \"type\": \"entity\",\n",
        "        \"name\": filename,\n",
        "        \"definition\": \"resolvedFrom/\" + filename,\n",
        "        \"contents\": contents\n",
        "      },\n",
        "      \"hasAttributes\": attributes,\n",
        "      \"version\": \"1.0.0.0\"\n",
        "    }\n",
        "    ent_definitions.append(d)\n",
        "    entity_model = {\"jsonSchemaSemanticVersion\": \"1.1.0\",\"imports\":ent_imports, \"definitions\":ent_definitions}\n",
        "    #print(entity_model)\n",
        "\n",
        "    \n",
        "    json_model = json.dumps(entity_model) \n",
        "\n",
        "    # convert to a dataframe\n",
        "    json_list = []\n",
        "    json_list.append(json_model)\n",
        "    df = spark.read.json(sc.parallelize(json_list))\n",
        "    #display(df)\n",
        "\n",
        "    entfilebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/c360data/' \n",
        "    entjsonpath = entfilebasepath + 'tempfolder'\n",
        "\n",
        "    df.coalesce(1).write.format('json').mode('overwrite').save(entjsonpath)\n",
        "    \n",
        "    # copy the model json file written in parts to a single model.json file\n",
        "    from notebookutils import mssparkutils\n",
        "    files = mssparkutils.fs.ls('/c360data/tempfolder')\n",
        "    for file1 in files:\n",
        "        if '.json' in file1.name:\n",
        "            srcfilename = '/c360data/tempfolder/' + file1.name\n",
        "            targetfilename = '/c360data/resolve/' + filename + '.cdm.json'\n",
        "            mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
        "            break\n",
        "    #delete the folder with parts file        \n",
        "    mssparkutils.fs.rm('/c360data/tempfolder',recurse=True)\n",
        "    #break\n",
        "\n",
        "\n",
        "#create the main manifest.json file\n",
        "imports = []\n",
        "entities = []\n",
        "\n",
        "for filename in filenames:\n",
        "    d = {\n",
        "      \"type\": \"LocalEntity\",\n",
        "      \"entityName\": filename,\n",
        "      \"entityPath\": \"resolve/\" + filename + '.cdm.json/' + filename,\n",
        "      \"dataPartitionPatterns\": [\n",
        "        {\n",
        "          \"name\": filename,\n",
        "          \"rootLocation\": 'synapse/workspaces/' + synapse_workspace_name + '/warehouse/c360_data.db/' + filename,\n",
        "          \"regularExpression\": \".+\\\\.parquet$\",\n",
        "          \"parameters\": [],\n",
        "          \"exhibitsTraits\": [\n",
        "            {\"traitReference\" : \"is.partition.format.parquet\"}\n",
        "          ]\n",
        "        }\n",
        "      ],\n",
        "      \"definitions\": get_ci_datedefinitions()\n",
        "    }\n",
        "    entities.append(d)\n",
        "\n",
        "manifest = {\"manifestName\": \"default\",\"entities\": entities,\"jsonSchemaSemanticVersion\": \"1.1.0\",\"imports\":imports}\n",
        "\n",
        "json_model = json.dumps(manifest) \n",
        "\n",
        "# convert to a dataframe\n",
        "json_list = []\n",
        "json_list.append(json_model)\n",
        "df = spark.read.json(sc.parallelize(json_list))\n",
        "#display(df)\n",
        "\n",
        "entfilebasepath = f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/c360data/' \n",
        "entjsonpath = entfilebasepath + 'tempfolder'\n",
        "\n",
        "df.coalesce(1).write.format('json').mode('overwrite').save(entjsonpath)\n",
        "\n",
        "# copy the model json file written in parts to a single model.json file\n",
        "from notebookutils import mssparkutils\n",
        "files = mssparkutils.fs.ls('/c360data/tempfolder')\n",
        "for file1 in files:\n",
        "    if '.json' in file1.name:\n",
        "        srcfilename = '/c360data/tempfolder/' + file1.name\n",
        "        targetfilename = '/c360data/' + 'default.manifest.cdm.json'\n",
        "        mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
        "        break\n",
        "#delete the folder with parts file        \n",
        "mssparkutils.fs.rm('/c360data/tempfolder',recurse=True)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "try:\n",
        "    mssparkutils.fs.rm('/default.manifest.cdm.json')\n",
        "    mssparkutils.fs.rm('/resolve',recurse=True)\n",
        "except:\n",
        "    pass\n",
        "    \n",
        "srcfilename = '/c360data/default.manifest.cdm.json'\n",
        "targetfilename = '/default.manifest.cdm.json'\n",
        "mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
        "\n",
        "# mssparkutils.fs.help()\n",
        "srcfilename = '/c360data/resolve'\n",
        "targetfilename = '/resolve'\n",
        "mssparkutils.fs.cp(srcfilename,targetfilename, True)\n",
        "\n",
        "mssparkutils.fs.rm('/c360data',recurse=True)"
      ]
    }
  ]
}