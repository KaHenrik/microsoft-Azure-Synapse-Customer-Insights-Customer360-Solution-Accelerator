{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\r\n",
        "\r\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read in Data from Azure Data Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "#update below variables with synapse adls name and container/filesystem name\n",
        "data_lake_account_name = \"\"\n",
        "file_system_name = \"data\"\n",
        "ci_instance_id  = \"\"\n",
        "# resident_file_name = \"residents.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "export_path = 'abfss://' + 'ciexport' + '@' + data_lake_account_name + '.dfs.core.windows.net/CustomerInsights_' + ci_instance_id + '/CustomersExport/Customer'\n",
        "folders = mssparkutils.fs.ls(export_path)\r\n",
        "\r\n",
        "isCSVFolder = False\r\n",
        "while isCSVFolder is False: \r\n",
        "    folder = folders[-1]  #get the latest folder from parent in a loop \r\n",
        "    folders = mssparkutils.fs.ls(folder.path)\r\n",
        "    if any(\".csv\" in s.path for s in folders): # break when we reach the leaf folder with csv files\r\n",
        "        isCSVFolder = True\r\n",
        "        break\r\n",
        "\r\n",
        "print(folder.path)\r\n",
        "\r\n",
        "df = spark.read.load(folder.path, format='csv', header=True)\r\n",
        "df = df.select(['CustomerId','sourcedata_residents_source1_cid','sourcedata_residents_source2_cid','SurveyEmail'])\r\n",
        "#display(df1.take(10))\r\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Customer profile ids\r\n",
        "# sql_str = '''select * from c360_data.customer_profile_ids'''\r\n",
        "sql_str = '''select CustomerId,sourcedata_residents_source1_cid as cid,SurveyEmail from \r\n",
        "c360_data.customer_profile_ids where sourcedata_residents_source1_cid is not null \r\n",
        "union all \r\n",
        "select CustomerId,sourcedata_residents_source2_cid as cid,SurveyEmail from c360_data.customer_profile_ids \r\n",
        "where sourcedata_residents_source2_cid is not null'''\r\n",
        "\r\n",
        "df_customer_profile_ids = spark.sql(sql_str)\r\n",
        "df_customer_profile_ids.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids_combined\")\r\n",
        "# display(df_customer_profile_ids.take(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true,
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "cid"
            ],
            "values": [
              "cid"
            ],
            "yLabel": "cid",
            "xLabel": "cid",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "# Initial Lease details\n",
        "sql_str = '''select cid,pid,uid, LeaseTerm as InitialLeaseTerm from \n",
        "(\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t \n",
        "where Type in ('Application')'''\n",
        "\n",
        "df_lease_initial = spark.sql(sql_str)\n",
        "#display(df_lease_initial)\n",
        "\n",
        "#Renewals\n",
        "sql_str = '''select cid,pid,uid, count(*) as num_renewals, \n",
        "case when count(*) >=1 then 'Y' else 'N' end as isRenewed,\n",
        "avg(Leaseterm) as avg_renewal_leaseterm from (\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t where Type = 'Renewal'\n",
        "group by cid,pid,uid'''\n",
        "\n",
        "df_lease_renewal = spark.sql(sql_str)\n",
        "#display(df_lease_renewal)\n",
        "\n",
        "# Moveout details\n",
        "sql_str = '''select cid,pid,uid, min(StartDate) as min_LeaseBeginDate, max(EndDate) as max_LeaseEndDate, \n",
        "max(MoveOutDate) as max_MoveOutDate, \n",
        "case when max(MoveOutDate) is null then 'N' else 'Y' end as isMovedOut,\n",
        "DATEDIFF(max(MoveOutDate),max(EndDate)) as diffMoveOutDays,\n",
        "case when DATEDIFF(max(MoveOutDate),max(EndDate)) > 30 then 'Y' else 'N' end as isEarlyMoveOut\n",
        "from \n",
        "(\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t \n",
        "group by cid,pid,uid'''\n",
        "\n",
        "df_lease_moveout = spark.sql(sql_str)\n",
        "#display(df_lease_moveout)\n",
        "\n",
        "df_leasedata = df_lease_initial.join(df_lease_renewal,[\"cid\",\"pid\",\"uid\"],how='left')\n",
        "df_leasedata = df_leasedata.join(df_lease_moveout,[\"cid\",\"pid\",\"uid\"],how='left')\n",
        "display(df_leasedata.take(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df_leasedata = df_leasedata.join(df_customer_profile_ids,[\"cid\"],how='left')\r\n",
        "df_leasedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# get workorder details\n",
        "import pyspark.sql.functions as F\n",
        "#from pyspark.sql.functions import regexp_replace, col\n",
        "import re\n",
        "\n",
        "sql_str = '''select cid, pid,uid,workorder_type, count(*) as num_workorders\n",
        "from \n",
        "(\n",
        "    select cid, pid,uid, \n",
        "    CONCAT('WO_',workorder_type) as workorder_type,\n",
        "    ServiceRequestDate, ServiceCompleteDate\n",
        "    from c360_data.workorders\n",
        ") t\n",
        "Group by cid,pid,uid,workorder_type'''\n",
        "\n",
        "df_workorders = spark.sql(sql_str)\n",
        "\n",
        "# remove any special characters in subcategory column\n",
        "df_workorders = df_workorders.withColumn(\"workorder_type\",F.regexp_replace(F.col(\"workorder_type\"), \"[^0-9a-zA-Z_$]+\", \"\"))\n",
        "\n",
        "# pivot rows into columns to get each work order subcategory as a column\n",
        "#https://stackoverflow.com/questions/33732346/spark-dataframe-transform-multiple-rows-to-column\n",
        "df_workorders = df_workorders.groupby(['cid','pid','uid']).pivot('workorder_type').max('num_workorders').fillna(0)\n",
        "\n",
        "#display(df_workorders.take(2))\n",
        "# df_workorders.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#get survey data\n",
        "sql_str = '''select cid,pid,concat(surveytype,'_',question) as Survey_Question, Avg(CAST(answer as INT)) as avg_SurveryAnswer from\n",
        "(\n",
        "    SELECT s.pid, ids.cid, s.sid,\n",
        "    s.question, s.answer,\n",
        "    s.surveytype\n",
        "    FROM c360_data.surveys s\n",
        "    inner join c360_data.customer_profile_ids_combined as ids on s.email = ids.SurveyEmail\n",
        ") t\n",
        "Group by cid, pid, Survey_Question'''\n",
        "\n",
        "df_surveydata = spark.sql(sql_str)\n",
        "\n",
        "#display(df_surveydata.take(2))\n",
        "\n",
        "# remove any special characters in Survey_Question column\n",
        "df_surveydata = df_surveydata.withColumn(\"Survey_Question\",F.regexp_replace(F.col(\"Survey_Question\"), \"[^0-9a-zA-Z_$]+\", \"\"))\n",
        "\n",
        "# pivot rows into columns to get each Survey_Question as a column\n",
        "df_surveydata = df_surveydata.groupby(['cid','pid']).pivot('Survey_Question').max('avg_SurveryAnswer').fillna(0)\n",
        "\n",
        "# remove any special characters in subcategory name in columns\n",
        "#df_surveydata = df_surveydata.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z_$]+\",\"\",col)) for col in df_surveydata.columns])\n",
        "\n",
        "#display(df_workorders.take(2))\n",
        "# df_surveydata.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_all_data = df_leasedata.join(df_workorders,[\"cid\",\"pid\",\"uid\"], how='left')\n",
        "df_all_data = df_all_data.join(df_surveydata,[\"cid\",\"pid\"], how='left')\n",
        "#display(df_all_data)\n",
        "df_all_data.write.mode(\"overwrite\").saveAsTable(\"c360_data.preparedinferencedata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.exit(\"c360_data.preparedinferencedata\") "
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}