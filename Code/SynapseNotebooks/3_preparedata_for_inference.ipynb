{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Imports"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from notebookutils import mssparkutils"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:18:14.4857123Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:14.5828089Z",
              "execution_finish_time": "2021-10-14T04:18:14.7272193Z"
            },
            "text/plain": "StatementMeta(spark1, 36, 11, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Data from Azure Data Lake"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#update below variables with synapse adls name and container/filesystem name\n",
        "data_lake_account_name = ''\n",
        "file_system_name = 'data'\n",
        "\n",
        "resident_file_name = \"residents.csv\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:18:14.5908865Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:14.8077922Z",
              "execution_finish_time": "2021-10-14T04:18:14.9550698Z"
            },
            "text/plain": "StatementMeta(spark1, 36, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sql(\"select CustomerId,sourcedata_residents_source1_cid,sourcedata_residents_source2_cid,SurveyEmail from ciexport.Customer\")\n",
        "df = df.select(['CustomerId','sourcedata_residents_source1_cid','sourcedata_residents_source2_cid','SurveyEmail'])\n",
        "\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:18:14.7849606Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:15.0333856Z",
              "execution_finish_time": "2021-10-14T04:18:18.8967798Z"
            },
            "text/plain": "StatementMeta(spark1, 36, 13, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Customer profile ids\n",
        "sql_str = '''select CustomerId, sourcedata_residents_source1_cid as cid, SurveyEmail \n",
        "from c360_data.customer_profile_ids where sourcedata_residents_source1_cid is not null \n",
        "union all \n",
        "select CustomerId, sourcedata_residents_source2_cid as cid, SurveyEmail from \n",
        "c360_data.customer_profile_ids where sourcedata_residents_source2_cid is not null''' \n",
        "\n",
        "df_customer_profile_ids = spark.sql(sql_str)\n",
        "df_customer_profile_ids.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids_combined\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:18:14.9857029Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:18.9775175Z",
              "execution_finish_time": "2021-10-14T04:18:22.8507195Z"
            },
            "text/plain": "StatementMeta(spark1, 36, 14, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Lease details\n",
        "sql_str = '''select cid,pid,uid, LeaseTerm as InitialLeaseTerm from \n",
        "(\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t \n",
        "where Type in ('Application')'''\n",
        "\n",
        "df_lease_initial = spark.sql(sql_str)\n",
        "#display(df_lease_initial)\n",
        "\n",
        "#Renewals\n",
        "sql_str = '''select cid,pid,uid, count(*) as num_renewals, \n",
        "case when count(*) >=1 then 'Y' else 'N' end as isRenewed,\n",
        "avg(Leaseterm) as avg_renewal_leaseterm from (\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t where Type = 'Renewal'\n",
        "group by cid,pid,uid'''\n",
        "\n",
        "df_lease_renewal = spark.sql(sql_str)\n",
        "#display(df_lease_renewal)\n",
        "\n",
        "# Moveout details\n",
        "sql_str = '''select cid,pid,uid, min(StartDate) as min_LeaseBeginDate, max(EndDate) as max_LeaseEndDate, \n",
        "max(MoveOutDate) as max_MoveOutDate, \n",
        "case when max(MoveOutDate) is null then 'N' else 'Y' end as isMovedOut,\n",
        "DATEDIFF(max(MoveOutDate),max(EndDate)) as diffMoveOutDays,\n",
        "case when DATEDIFF(max(MoveOutDate),max(EndDate)) > 30 then 'Y' else 'N' end as isEarlyMoveOut\n",
        "from \n",
        "(\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t \n",
        "group by cid,pid,uid'''\n",
        "\n",
        "df_lease_moveout = spark.sql(sql_str)\n",
        "#display(df_lease_moveout)\n",
        "\n",
        "df_leasedata = df_lease_initial.join(df_lease_renewal,[\"cid\",\"pid\",\"uid\"],how='left')\n",
        "df_leasedata = df_leasedata.join(df_lease_moveout,[\"cid\",\"pid\",\"uid\"],how='left')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:18:15.4387207Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:22.9343236Z",
              "execution_finish_time": "2021-10-14T04:18:23.9760393Z"
            },
            "text/plain": "StatementMeta(spark1, 36, 15, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_leasedata = df_leasedata.join(df_customer_profile_ids,[\"cid\"],how='left')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:18:15.7785202Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:24.1059319Z",
              "execution_finish_time": "2021-10-14T04:18:24.2567763Z"
            },
            "text/plain": "StatementMeta(spark1, 36, 16, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get workorder details\n",
        "import pyspark.sql.functions as F\n",
        "#from pyspark.sql.functions import regexp_replace, col\n",
        "import re\n",
        "\n",
        "sql_str = '''select cid, pid,uid,workorder_type, count(*) as num_workorders\n",
        "from \n",
        "(\n",
        "    select cid, pid,uid, \n",
        "    CONCAT('WO_',workorder_type) as workorder_type,\n",
        "    ServiceRequestDate, ServiceCompleteDate\n",
        "    from c360_data.workorders\n",
        ") t\n",
        "Group by cid,pid,uid,workorder_type'''\n",
        "\n",
        "df_workorders = spark.sql(sql_str)\n",
        "\n",
        "# remove any special characters in subcategory column\n",
        "df_workorders = df_workorders.withColumn(\"workorder_type\",F.regexp_replace(F.col(\"workorder_type\"), \"[^0-9a-zA-Z_$]+\", \"\"))\n",
        "\n",
        "# pivot rows into columns to get each work order subcategory as a column\n",
        "#https://stackoverflow.com/questions/33732346/spark-dataframe-transform-multiple-rows-to-column\n",
        "df_workorders = df_workorders.groupby(['cid','pid','uid']).pivot('workorder_type').max('num_workorders').fillna(0)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:18:15.9261434Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:24.3564272Z",
              "execution_finish_time": "2021-10-14T04:18:26.1475875Z"
            },
            "text/plain": "StatementMeta(spark1, 36, 17, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#get survey data\n",
        "sql_str = '''select cid,pid,concat(surveytype,'_',question) as Survey_Question, Avg(CAST(answer as INT)) as avg_SurveryAnswer from\n",
        "(\n",
        "    SELECT s.pid, ids.cid, s.sid,\n",
        "    s.question, s.answer,\n",
        "    s.surveytype\n",
        "    FROM c360_data.surveys s\n",
        "    inner join c360_data.customer_profile_ids_combined as ids on s.email = ids.SurveyEmail\n",
        ") t\n",
        "Group by cid, pid, Survey_Question'''\n",
        "\n",
        "df_surveydata = spark.sql(sql_str)\n",
        "\n",
        "#display(df_surveydata.take(2))\n",
        "\n",
        "# remove any special characters in Survey_Question column\n",
        "df_surveydata = df_surveydata.withColumn(\"Survey_Question\",F.regexp_replace(F.col(\"Survey_Question\"), \"[^0-9a-zA-Z_$]+\", \"\"))\n",
        "\n",
        "# pivot rows into columns to get each Survey_Question as a column\n",
        "df_surveydata = df_surveydata.groupby(['cid','pid']).pivot('Survey_Question').max('avg_SurveryAnswer').fillna(0)\n",
        "\n",
        "# remove any special characters in subcategory name in columns\n",
        "#df_surveydata = df_surveydata.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z_$]+\",\"\",col)) for col in df_surveydata.columns])\n",
        "\n",
        "#display(df_workorders.take(2))\n",
        "# df_surveydata.columns"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 36,
              "statement_id": 18,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2021-10-14T04:18:16.0613707Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:18:26.2274817Z",
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(spark1, 36, 18, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_data = df_leasedata.join(df_workorders,[\"cid\",\"pid\",\"uid\"], how='left')\r\n",
        "df_all_data = df_all_data.join(df_surveydata,[\"cid\",\"pid\"], how='left')\r\n",
        "#display(df_all_data)\r\n",
        "df_all_data.write.mode(\"overwrite\").saveAsTable(\"c360_data.preparedinferencedata\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2021-10-14T04:18:16.1758309Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mssparkutils.notebook.exit(\"c360_data.preparedinferencedata\") "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2021-10-14T04:18:16.2982618Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}