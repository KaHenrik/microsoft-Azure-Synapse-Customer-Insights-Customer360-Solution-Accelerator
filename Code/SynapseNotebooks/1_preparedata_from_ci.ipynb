{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:11:38.0099891Z",
              "execution_start_time": "2021-10-14T04:11:37.8624512Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0654013Z",
              "session_id": 34,
              "session_start_time": "2021-10-14T04:11:07.1120516Z",
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from notebookutils import mssparkutils\n",
        "import pyspark.sql.functions as F\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read in Data from Azure Data Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:11:38.2561506Z",
              "execution_start_time": "2021-10-14T04:11:38.0948446Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0727055Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#update below variables with synapse adls name and container/filesystem name\n",
        "data_lake_account_name = '' # Synapse Workspace ADLS\n",
        "file_system_name = 'data'\n",
        "resident_file_name = \"residents.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:11:57.0877453Z",
              "execution_start_time": "2021-10-14T04:11:38.3354165Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0739115Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = spark.sql(\"select CustomerId,sourcedata_residents_source1_cid,sourcedata_residents_source2_cid,SurveyEmail from ciexport.Customer\")\n",
        "df = df.select(['CustomerId','sourcedata_residents_source1_cid','sourcedata_residents_source2_cid','SurveyEmail'])\n",
        "#display(df1.take(10))\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:12:01.0573421Z",
              "execution_start_time": "2021-10-14T04:11:57.1717801Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0752351Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sql_str = \"Select CustomerId, sourcedata_residents_source1_cid as cid, SurveyEmail from c360_data.customer_profile_ids \\\n",
        " where sourcedata_residents_source1_cid is not null \\\n",
        " union all \\\n",
        " select CustomerID, sourcedata_residents_source2_cid as cid, SurveyEmail from c360_data.customer_profile_ids \\\n",
        " where sourcedata_residents_source2_cid is not null\"\n",
        "\n",
        "df_customer_profile_ids = spark.sql(sql_str)\n",
        "df_customer_profile_ids.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids_combined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:12:02.2121124Z",
              "execution_start_time": "2021-10-14T04:12:01.1439418Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0765208Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initial Lease details\n",
        "sql_str = '''select cid,pid,uid, LeaseTerm as InitialLeaseTerm from \n",
        "(\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t \n",
        "where Type in ('Application')'''\n",
        "\n",
        "df_lease_initial = spark.sql(sql_str)\n",
        "#display(df_lease_initial)\n",
        "\n",
        "#Renewals\n",
        "sql_str = '''select cid,pid,uid, count(*) as num_renewals, \n",
        "case when count(*) >=1 then 'Y' else 'N' end as isRenewed,\n",
        "avg(Leaseterm) as avg_renewal_leaseterm from (\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t where Type = 'Renewal'\n",
        "group by cid,pid,uid'''\n",
        "\n",
        "df_lease_renewal = spark.sql(sql_str)\n",
        "#display(df_lease_renewal)\n",
        "\n",
        "# Moveout details\n",
        "sql_str = '''select cid,pid,uid, min(StartDate) as min_LeaseBeginDate, max(EndDate) as max_LeaseEndDate, \n",
        "max(MoveOutDate) as max_MoveOutDate, \n",
        "case when max(MoveOutDate) is null then 'N' else 'Y' end as isMovedOut,\n",
        "DATEDIFF(max(MoveOutDate),max(EndDate)) as diffMoveOutDays,\n",
        "case when DATEDIFF(max(MoveOutDate),max(EndDate)) > 30 then 'Y' else 'N' end as isEarlyMoveOut\n",
        "from \n",
        "(\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\n",
        "    from c360_data.leases as l \n",
        ") t \n",
        "group by cid,pid,uid'''\n",
        "\n",
        "df_lease_moveout = spark.sql(sql_str)\n",
        "#display(df_lease_moveout)\n",
        "\n",
        "df_leasedata = df_lease_initial.join(df_lease_renewal,[\"cid\",\"pid\",\"uid\"],how='left')\n",
        "df_leasedata = df_leasedata.join(df_lease_moveout,[\"cid\",\"pid\",\"uid\"],how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:12:07.5400128Z",
              "execution_start_time": "2021-10-14T04:12:02.295231Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0778236Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get workorder details\n",
        "sql_str = '''select cid, pid,uid,workorder_type, count(*) as num_workorders\n",
        "from \n",
        "(\n",
        "    select cid, pid,uid, \n",
        "    CONCAT('WO_',workorder_type) as workorder_type,\n",
        "    ServiceRequestDate, ServiceCompleteDate\n",
        "    from c360_data.workorders\n",
        ") t\n",
        "Group by cid,pid,uid,workorder_type'''\n",
        "\n",
        "df_workorders = spark.sql(sql_str)\n",
        "\n",
        "# remove any special characters in subcategory column\n",
        "df_workorders = df_workorders.withColumn(\"workorder_type\",F.regexp_replace(F.col(\"workorder_type\"), \"[^0-9a-zA-Z_$]+\", \"\"))\n",
        "\n",
        "# pivot rows into columns to get each work order subcategory as a column\n",
        "#https://stackoverflow.com/questions/33732346/spark-dataframe-transform-multiple-rows-to-column\n",
        "df_workorders = df_workorders.groupby(['cid','pid','uid']).pivot('workorder_type').max('num_workorders').fillna(0)\n",
        "\n",
        "#display(df_workorders.take(2))\n",
        "# df_workorders.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:12:12.9118892Z",
              "execution_start_time": "2021-10-14T04:12:07.6285324Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0796184Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#get survey data\n",
        "sql_str = '''select cid,pid,concat(surveytype,'_',question) as Survey_Question, Avg(CAST(answer as INT)) as avg_SurveryAnswer from\n",
        "(\n",
        "    SELECT s.pid, ids.cid, s.sid,\n",
        "    s.question, s.answer,\n",
        "    s.surveytype\n",
        "    FROM c360_data.surveys s\n",
        "    inner join c360_data.customer_profile_ids_combined as ids on s.email = ids.SurveyEmail\n",
        ") t\n",
        "Group by cid, pid, Survey_Question'''\n",
        "\n",
        "df_surveydata = spark.sql(sql_str)\n",
        "\n",
        "# remove any special characters in Survey_Question column\n",
        "df_surveydata = df_surveydata.withColumn(\"Survey_Question\",F.regexp_replace(F.col(\"Survey_Question\"), \"[^0-9a-zA-Z_$]+\", \"\"))\n",
        "\n",
        "# pivot rows into columns to get each Survey_Question as a column\n",
        "df_surveydata = df_surveydata.groupby(['cid','pid']).pivot('Survey_Question').max('avg_SurveryAnswer').fillna(0)\n",
        "\n",
        "# remove any special characters in subcategory name in columns\n",
        "#df_surveydata = df_surveydata.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z_$]+\",\"\",col)) for col in df_surveydata.columns])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:12:29.6639701Z",
              "execution_start_time": "2021-10-14T04:12:12.9936879Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0811522Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_all_data = df_leasedata.join(df_workorders,[\"cid\",\"pid\",\"uid\"], how='left')\n",
        "df_all_data = df_all_data.join(df_surveydata,[\"cid\",\"pid\"], how='left')\n",
        "#display(df_all_data)\n",
        "df_all_data.write.mode(\"overwrite\").saveAsTable(\"c360_data.prepareddata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-14T04:12:29.9028012Z",
              "execution_start_time": "2021-10-14T04:12:29.7479496Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0829773Z",
              "session_id": 34,
              "session_start_time": null,
              "spark_pool": "spark1",
              "state": "finished",
              "statement_id": 9
            },
            "text/plain": [
              "StatementMeta(spark1, 34, 9, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ExitValue: c360_data.prepareddata"
          ]
        }
      ],
      "source": [
        "mssparkutils.notebook.exit(\"c360_data.prepareddata\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
