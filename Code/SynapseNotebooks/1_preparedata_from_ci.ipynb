{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation.\r\n",
        "\r\n",
        "Licensed under the MIT License."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Imports"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from notebookutils import mssparkutils\r\n",
        "import pyspark.sql.functions as F\r\n",
        "import re"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0654013Z",
              "session_start_time": "2021-10-14T04:11:07.1120516Z",
              "execution_start_time": "2021-10-14T04:11:37.8624512Z",
              "execution_finish_time": "2021-10-14T04:11:38.0099891Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Data from Azure Data Lake"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#update below variables with synapse adls name and container/filesystem name\n",
        "data_lake_account_name = ''\n",
        "file_system_name = 'data'\n",
        "resident_file_name = \"residents.csv\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0727055Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:11:38.0948446Z",
              "execution_finish_time": "2021-10-14T04:11:38.2561506Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sql(\"select CustomerId,sourcedata_residents_source1_cid,sourcedata_residents_source2_cid,SurveyEmail from ciexport.Customer\")\n",
        "df = df.select(['CustomerId','sourcedata_residents_source1_cid','sourcedata_residents_source2_cid','SurveyEmail'])\n",
        "#display(df1.take(10))\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0739115Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:11:38.3354165Z",
              "execution_finish_time": "2021-10-14T04:11:57.0877453Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql_str = \"Select CustomerId, sourcedata_residents_source1_cid as cid, SurveyEmail from c360_data.customer_profile_ids \\\r\n",
        " where sourcedata_residents_source1_cid is not null \\\r\n",
        " union all \\\r\n",
        " select CustomerID, sourcedata_residents_source2_cid as cid, SurveyEmail from c360_data.customer_profile_ids \\\r\n",
        " where sourcedata_residents_source2_cid is not null\"\r\n",
        "\r\n",
        "df_customer_profile_ids = spark.sql(sql_str)\r\n",
        "df_customer_profile_ids.write.mode(\"overwrite\").saveAsTable(\"c360_data.customer_profile_ids_combined\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0752351Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:11:57.1717801Z",
              "execution_finish_time": "2021-10-14T04:12:01.0573421Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Lease details\r\n",
        "sql_str = '''select cid,pid,uid, LeaseTerm as InitialLeaseTerm from \r\n",
        "(\r\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\r\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\r\n",
        "    from c360_data.leases as l \r\n",
        ") t \r\n",
        "where Type in ('Application')'''\r\n",
        "\r\n",
        "df_lease_initial = spark.sql(sql_str)\r\n",
        "#display(df_lease_initial)\r\n",
        "\r\n",
        "#Renewals\r\n",
        "sql_str = '''select cid,pid,uid, count(*) as num_renewals, \r\n",
        "case when count(*) >=1 then 'Y' else 'N' end as isRenewed,\r\n",
        "avg(Leaseterm) as avg_renewal_leaseterm from (\r\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\r\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\r\n",
        "    from c360_data.leases as l \r\n",
        ") t where Type = 'Renewal'\r\n",
        "group by cid,pid,uid'''\r\n",
        "\r\n",
        "df_lease_renewal = spark.sql(sql_str)\r\n",
        "#display(df_lease_renewal)\r\n",
        "\r\n",
        "# Moveout details\r\n",
        "sql_str = '''select cid,pid,uid, min(StartDate) as min_LeaseBeginDate, max(EndDate) as max_LeaseEndDate, \r\n",
        "max(MoveOutDate) as max_MoveOutDate, \r\n",
        "case when max(MoveOutDate) is null then 'N' else 'Y' end as isMovedOut,\r\n",
        "DATEDIFF(max(MoveOutDate),max(EndDate)) as diffMoveOutDays,\r\n",
        "case when DATEDIFF(max(MoveOutDate),max(EndDate)) > 30 then 'Y' else 'N' end as isEarlyMoveOut\r\n",
        "from \r\n",
        "(\r\n",
        "    select l.cid,l.pid,l.uid,l.lid,l.Type,l.LeaseTerm,\r\n",
        "    SignedDate, StartDate, EndDate, MoveOutDate\r\n",
        "    from c360_data.leases as l \r\n",
        ") t \r\n",
        "group by cid,pid,uid'''\r\n",
        "\r\n",
        "df_lease_moveout = spark.sql(sql_str)\r\n",
        "#display(df_lease_moveout)\r\n",
        "\r\n",
        "df_leasedata = df_lease_initial.join(df_lease_renewal,[\"cid\",\"pid\",\"uid\"],how='left')\r\n",
        "df_leasedata = df_leasedata.join(df_lease_moveout,[\"cid\",\"pid\",\"uid\"],how='left')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0765208Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:12:01.1439418Z",
              "execution_finish_time": "2021-10-14T04:12:02.2121124Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get workorder details\r\n",
        "sql_str = '''select cid, pid,uid,workorder_type, count(*) as num_workorders\r\n",
        "from \r\n",
        "(\r\n",
        "    select cid, pid,uid, \r\n",
        "    CONCAT('WO_',workorder_type) as workorder_type,\r\n",
        "    ServiceRequestDate, ServiceCompleteDate\r\n",
        "    from c360_data.workorders\r\n",
        ") t\r\n",
        "Group by cid,pid,uid,workorder_type'''\r\n",
        "\r\n",
        "df_workorders = spark.sql(sql_str)\r\n",
        "\r\n",
        "# remove any special characters in subcategory column\r\n",
        "df_workorders = df_workorders.withColumn(\"workorder_type\",F.regexp_replace(F.col(\"workorder_type\"), \"[^0-9a-zA-Z_$]+\", \"\"))\r\n",
        "\r\n",
        "# pivot rows into columns to get each work order subcategory as a column\r\n",
        "#https://stackoverflow.com/questions/33732346/spark-dataframe-transform-multiple-rows-to-column\r\n",
        "df_workorders = df_workorders.groupby(['cid','pid','uid']).pivot('workorder_type').max('num_workorders').fillna(0)\r\n",
        "\r\n",
        "#display(df_workorders.take(2))\r\n",
        "# df_workorders.columns"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0778236Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:12:02.295231Z",
              "execution_finish_time": "2021-10-14T04:12:07.5400128Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get survey data\r\n",
        "sql_str = '''select cid,pid,concat(surveytype,'_',question) as Survey_Question, Avg(CAST(answer as INT)) as avg_SurveryAnswer from\r\n",
        "(\r\n",
        "    SELECT s.pid, ids.cid, s.sid,\r\n",
        "    s.question, s.answer,\r\n",
        "    s.surveytype\r\n",
        "    FROM c360_data.surveys s\r\n",
        "    inner join c360_data.customer_profile_ids_combined as ids on s.email = ids.SurveyEmail\r\n",
        ") t\r\n",
        "Group by cid, pid, Survey_Question'''\r\n",
        "\r\n",
        "df_surveydata = spark.sql(sql_str)\r\n",
        "\r\n",
        "# remove any special characters in Survey_Question column\r\n",
        "df_surveydata = df_surveydata.withColumn(\"Survey_Question\",F.regexp_replace(F.col(\"Survey_Question\"), \"[^0-9a-zA-Z_$]+\", \"\"))\r\n",
        "\r\n",
        "# pivot rows into columns to get each Survey_Question as a column\r\n",
        "df_surveydata = df_surveydata.groupby(['cid','pid']).pivot('Survey_Question').max('avg_SurveryAnswer').fillna(0)\r\n",
        "\r\n",
        "# remove any special characters in subcategory name in columns\r\n",
        "#df_surveydata = df_surveydata.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z_$]+\",\"\",col)) for col in df_surveydata.columns])\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0796184Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:12:07.6285324Z",
              "execution_finish_time": "2021-10-14T04:12:12.9118892Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_data = df_leasedata.join(df_workorders,[\"cid\",\"pid\",\"uid\"], how='left')\r\n",
        "df_all_data = df_all_data.join(df_surveydata,[\"cid\",\"pid\"], how='left')\r\n",
        "#display(df_all_data)\r\n",
        "df_all_data.write.mode(\"overwrite\").saveAsTable(\"c360_data.prepareddata\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0811522Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:12:12.9936879Z",
              "execution_finish_time": "2021-10-14T04:12:29.6639701Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mssparkutils.notebook.exit(\"c360_data.prepareddata\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 34,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-14T04:11:07.0829773Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-14T04:12:29.7479496Z",
              "execution_finish_time": "2021-10-14T04:12:29.9028012Z"
            },
            "text/plain": "StatementMeta(spark1, 34, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExitValue: c360_data.prepareddata"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}